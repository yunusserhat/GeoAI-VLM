{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8345af",
   "metadata": {},
   "source": [
    "# GeoAI-VLM v0.2.0 — New Features Demo\n",
    "\n",
    "This notebook demonstrates the **new features** added in v0.2.0:\n",
    "\n",
    "1. **Multimodal Embedding** — Generate semantically rich vectors from text and images using Qwen3-VL-Embedding  \n",
    "2. **Vector Search** — Build and query searchable indexes with ChromaDB or FAISS  \n",
    "3. **Semantic Clustering** — K-Means clustering of VLM descriptions with TF-IDF keywords  \n",
    "4. **Spatial Autocorrelation** — Global and Local Moran's I analysis  \n",
    "5. **Visualization** — Elbow curves, cluster maps, LISA maps, category distributions  \n",
    "6. **Data Preparation** — Parse, merge, and construct embedding text from pipeline output  \n",
    "\n",
    "> **Prerequisites**: Install GeoAI-VLM v0.2.0 (`pip install -e .` from the repo root).  \n",
    "> Sections 1–5 work **offline** with synthetic data — no GPU or API key needed.  \n",
    "> Section 6 shows the full pipeline requiring a Mapillary API key and a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Create a reusable output directory\n",
    "OUTPUT_DIR = Path(\"./demo_v020_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"GeoAI-VLM v0.2.0 demo ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39103a",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Synthetic Dataset\n",
    "\n",
    "We create a small synthetic GeoDataFrame that mimics real GeoAI-VLM pipeline output.\n",
    "This lets us exercise every new module without needing a GPU or API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "N = 60\n",
    "\n",
    "land_uses = [\"residential\", \"commercial\", \"mixed\", \"institutional\", \"green_space\"]\n",
    "street_types = [\"arterial\", \"collector\", \"local\", \"pedestrian\", \"alley\"]\n",
    "characters = [\"busy\", \"quiet\", \"touristic\", \"residential\", \"industrial\"]\n",
    "\n",
    "narratives = [\n",
    "    \"A bustling commercial street lined with shops and cafés under colorful awnings.\",\n",
    "    \"A quiet residential alley with potted plants and laundry hanging between buildings.\",\n",
    "    \"A wide arterial road with modern office buildings and a tram line in the median.\",\n",
    "    \"A narrow pedestrian lane with cobblestones, leading to a historic mosque.\",\n",
    "    \"A tree-lined boulevard next to a public park with benches and a fountain.\",\n",
    "    \"An industrial zone with warehouses, trucks, and a railway crossing.\",\n",
    "    \"A touristic waterfront promenade overlooking the Bosphorus with ferries docked.\",\n",
    "    \"A mixed-use neighborhood with ground-floor shops and apartments above.\",\n",
    "    \"A collectors road connecting suburban housing blocks to the main highway.\",\n",
    "    \"A green institutional campus with university buildings surrounded by gardens.\",\n",
    "]\n",
    "\n",
    "tag_pool = [\"urban\", \"green\", \"historic\", \"modern\", \"waterfront\", \"dense\",\n",
    "            \"transit\", \"cultural\", \"quiet\", \"noisy\", \"pedestrian\", \"car-oriented\"]\n",
    "\n",
    "data = {\n",
    "    \"image_id\": [f\"img_{i:04d}\" for i in range(N)],\n",
    "    \"scene_narrative\": [narratives[i % len(narratives)] for i in range(N)],\n",
    "    \"semantic_tags\": [\n",
    "        \", \".join(rng.choice(tag_pool, size=4, replace=False).tolist())\n",
    "        for _ in range(N)\n",
    "    ],\n",
    "    \"land_use_primary\": rng.choice(land_uses, N).tolist(),\n",
    "    \"street_type\": rng.choice(street_types, N).tolist(),\n",
    "    \"place_character\": rng.choice(characters, N).tolist(),\n",
    "    \"lat\": (41.005 + rng.rand(N) * 0.01).tolist(),\n",
    "    \"lon\": (28.975 + rng.rand(N) * 0.01).tolist(),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(df[\"lon\"], df[\"lat\"])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "print(f\"Synthetic dataset: {len(gdf)} rows, {len(gdf.columns)} columns\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba010e66",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preparation\n",
    "\n",
    "The `preparation` module helps parse VLM JSON output, merge data sources, and\n",
    "build composite embedding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from geoai_vlm.preparation import (\n",
    "    parse_vlm_descriptions,\n",
    "    merge_data_sources,\n",
    "    extract_image_id,\n",
    "    build_embedding_text,\n",
    ")\n",
    "\n",
    "# --- parse_vlm_descriptions ---\n",
    "# Simulate a DataFrame with a JSON-encoded description column\n",
    "raw_descriptions = gdf[[\"image_id\"]].copy()\n",
    "raw_descriptions[\"vlm_description\"] = gdf.apply(\n",
    "    lambda r: json.dumps({\n",
    "        \"scene_narrative\": r[\"scene_narrative\"],\n",
    "        \"semantic_tags\": r[\"semantic_tags\"],\n",
    "        \"land_use_primary\": r[\"land_use_primary\"],\n",
    "    }),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "parsed = parse_vlm_descriptions(\n",
    "    raw_descriptions,\n",
    "    description_column=\"vlm_description\",\n",
    "    target_field=\"scene_narrative\",\n",
    ")\n",
    "print(\"Parsed descriptions:\")\n",
    "parsed[[\"image_id\", \"parsed_description\"]].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ffdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- extract_image_id ---\n",
    "# Pull numeric IDs from file-path strings\n",
    "paths = pd.Series([\n",
    "    \"/data/mapillary/1234567890.jpg\",\n",
    "    \"/data/mapillary/9876543210.jpg\",\n",
    "    \"/data/mapillary/1111111111.jpg\",\n",
    "])\n",
    "ids = extract_image_id(paths)\n",
    "print(\"Extracted IDs:\")\n",
    "print(ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- merge_data_sources ---\n",
    "metadata = gdf[[\"image_id\", \"lat\", \"lon\"]].copy()\n",
    "descriptions = gdf[[\"image_id\", \"scene_narrative\", \"semantic_tags\"]].copy()\n",
    "classifiers = gdf[[\"image_id\", \"land_use_primary\", \"street_type\"]].copy()\n",
    "\n",
    "unified = merge_data_sources(\n",
    "    metadata,\n",
    "    descriptions,\n",
    "    classifiers=classifiers,\n",
    "    merge_on=\"image_id\",\n",
    ")\n",
    "print(f\"Merged columns: {list(unified.columns)}\")\n",
    "unified.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88381215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build_embedding_text ---\n",
    "# Concatenate multiple columns into a single text for embedding\n",
    "gdf[\"embedding_text\"] = build_embedding_text(\n",
    "    gdf,\n",
    "    columns=[\"scene_narrative\", \"semantic_tags\", \"place_character\"],\n",
    "    separator=\" | \",\n",
    ")\n",
    "print(\"Embedding text sample:\")\n",
    "print(gdf[\"embedding_text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05370d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Semantic Clustering\n",
    "\n",
    "The `SemanticClusterer` generates embeddings, runs K-Means, extracts TF-IDF\n",
    "keywords per cluster, and profiles GeoAI categories.\n",
    "\n",
    "> Below we use a lightweight **mock embedder** to avoid downloading a model.\n",
    "> In production, replace it with `ImageEmbedder()` (see Section 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef66440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoai_vlm.clustering import SemanticClusterer, ClusterConfig\n",
    "\n",
    "\n",
    "# --- Lightweight mock embedder (no GPU needed) ---\n",
    "class DemoEmbedder:\n",
    "    \"\"\"Returns deterministic random embeddings for demo purposes.\"\"\"\n",
    "    instruction = \"demo\"\n",
    "\n",
    "    def embed_texts(self, texts, **kw):\n",
    "        rng = np.random.RandomState(len(texts))\n",
    "        emb = rng.randn(len(texts), 128).astype(np.float32)\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        return emb / np.where(norms == 0, 1, norms)\n",
    "\n",
    "\n",
    "embedder = DemoEmbedder()\n",
    "\n",
    "config = ClusterConfig(\n",
    "    n_clusters=5,\n",
    "    embedding_columns=[\"scene_narrative\", \"semantic_tags\", \"place_character\"],\n",
    "    n_keywords=8,\n",
    ")\n",
    "clusterer = SemanticClusterer(embedder=embedder, config=config)\n",
    "\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720924b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find optimal k with the elbow method ---\n",
    "k_values, inertias = clusterer.find_optimal_k(gdf, k_range=(2, 10))\n",
    "\n",
    "print(\"k  | Inertia\")\n",
    "for k, inertia in zip(k_values, inertias):\n",
    "    print(f\"{k:>2} | {inertia:,.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05584685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run clustering ---\n",
    "gdf_clustered = clusterer.cluster(gdf, n_clusters=5)\n",
    "\n",
    "print(f\"\\nCluster distribution:\")\n",
    "print(gdf_clustered[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5271af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract keywords per cluster ---\n",
    "gdf_clustered[\"embedding_text\"] = clusterer.build_embedding_text(gdf_clustered)\n",
    "keywords = clusterer.extract_keywords(gdf_clustered)\n",
    "\n",
    "print(\"Top keywords per cluster:\")\n",
    "for cid, kw_list in keywords.items():\n",
    "    print(f\"  Cluster {cid}: {', '.join(kw_list[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Category analysis ---\n",
    "profiles = clusterer.analyze_categories(\n",
    "    gdf_clustered,\n",
    "    category_columns=[\"land_use_primary\", \"street_type\", \"place_character\"],\n",
    ")\n",
    "\n",
    "for cid, profile in profiles.items():\n",
    "    print(f\"Cluster {cid} (n={profile['size']}):\")\n",
    "    for key, val in profile.items():\n",
    "        if key != \"size\":\n",
    "            print(f\"    {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf4dda",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Visualization\n",
    "\n",
    "Built-in plotting functions for quick exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19290e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # use 'TkAgg' or remove this line for interactive display\n",
    "\n",
    "from geoai_vlm.visualization import (\n",
    "    plot_elbow_curve,\n",
    "    plot_cluster_map,\n",
    "    plot_category_distribution,\n",
    "    generate_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cf20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Elbow curve ---\n",
    "fig_elbow = plot_elbow_curve(inertias, k_range=k_values, optimal_k=5)\n",
    "fig_elbow.savefig(OUTPUT_DIR / \"elbow_curve.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved elbow_curve.png\")\n",
    "fig_elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9daa444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cluster map ---\n",
    "fig_map = plot_cluster_map(gdf_clustered)\n",
    "fig_map.savefig(OUTPUT_DIR / \"cluster_map.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved cluster_map.png\")\n",
    "fig_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c75872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Category distribution ---\n",
    "fig_cats = plot_category_distribution(\n",
    "    gdf_clustered,\n",
    "    category_columns=[\"land_use_primary\", \"street_type\"],\n",
    ")\n",
    "fig_cats.savefig(OUTPUT_DIR / \"category_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved category_distribution.png\")\n",
    "fig_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a45ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Markdown report ---\n",
    "report = generate_report(gdf_clustered, keywords)\n",
    "print(report[:600])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8ebae",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Spatial Autocorrelation\n",
    "\n",
    "Global and Local Moran's I reveal whether clusters are spatially concentrated\n",
    "or randomly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21037e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoai_vlm.spatial import SpatialAnalyzer, MoranResult\n",
    "\n",
    "sa = SpatialAnalyzer(k_neighbors=8)\n",
    "\n",
    "# --- Global Moran's I ---\n",
    "global_results = sa.moran_global(gdf_clustered)\n",
    "\n",
    "print(\"Global Moran's I per cluster:\")\n",
    "print(f\"{'Cluster':>8}  {'I':>8}  {'E[I]':>8}  {'z':>8}  {'p':>8}\")\n",
    "for cid, mr in global_results.items():\n",
    "    sig = \"*\" if mr.p_value < 0.05 else \" \"\n",
    "    print(f\"{cid:>8}  {mr.I:>8.4f}  {mr.expected_I:>8.4f}  {mr.z_score:>8.3f}  {mr.p_value:>8.4f} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local Moran's I (LISA) ---\n",
    "gdf_lisa = sa.moran_local(gdf_clustered)\n",
    "\n",
    "print(f\"Significant LISA observations: {gdf_lisa['lisa_significant'].sum()} / {len(gdf_lisa)}\")\n",
    "print(f\"\\nLISA cluster distribution:\")\n",
    "print(gdf_lisa[\"lisa_cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LISA map ---\n",
    "from geoai_vlm.visualization import plot_lisa_map\n",
    "\n",
    "fig_lisa = plot_lisa_map(gdf_lisa)\n",
    "fig_lisa.savefig(OUTPUT_DIR / \"lisa_map.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved lisa_map.png\")\n",
    "fig_lisa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d217c5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Vector Search\n",
    "\n",
    "Build a searchable vector database from VLM descriptions using ChromaDB or FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba95983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoai_vlm.vectorstore import (\n",
    "    ChromaVectorStore,\n",
    "    FAISSVectorStore,\n",
    "    VectorDB,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b9d82",
   "metadata": {},
   "source": [
    "### 5a. Low-Level — ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic embeddings for all rows\n",
    "embeddings = embedder.embed_texts(gdf[\"scene_narrative\"].tolist())\n",
    "ids = gdf[\"image_id\"].tolist()\n",
    "metadatas = [\n",
    "    {\"land_use\": row[\"land_use_primary\"], \"street_type\": row[\"street_type\"]}\n",
    "    for _, row in gdf.iterrows()\n",
    "]\n",
    "\n",
    "# Create a persistent ChromaDB store\n",
    "chroma_store = ChromaVectorStore(\n",
    "    persist_directory=str(OUTPUT_DIR / \"chroma_db\"),\n",
    "    collection_name=\"demo_places\",\n",
    ")\n",
    "chroma_store.add(embeddings=embeddings, ids=ids, metadatas=metadatas)\n",
    "print(f\"ChromaDB: {chroma_store.count()} vectors stored\")\n",
    "\n",
    "# Query: find 5 most similar to the first image\n",
    "results = chroma_store.query(query_embedding=embeddings[0], n_results=5)\n",
    "print(f\"\\nTop-5 similar to {ids[0]}:\")\n",
    "for i, (rid, dist) in enumerate(zip(results[\"ids\"], results[\"distances\"])):\n",
    "    meta = results[\"metadatas\"][i] if results[\"metadatas\"] else {}\n",
    "    print(f\"  {i+1}. {rid} (dist={dist:.4f}) — {meta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13b32",
   "metadata": {},
   "source": [
    "### 5b. Low-Level — FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-memory FAISS index\n",
    "faiss_store = FAISSVectorStore(dimension=embeddings.shape[1])\n",
    "faiss_store.add(embeddings=embeddings, ids=ids, metadatas=metadatas)\n",
    "print(f\"FAISS: {faiss_store.count()} vectors stored\")\n",
    "\n",
    "# Query\n",
    "results_f = faiss_store.query(query_embedding=embeddings[0], n_results=5)\n",
    "print(f\"\\nTop-5 similar (FAISS):\")\n",
    "for i, rid in enumerate(results_f[\"ids\"]):\n",
    "    print(f\"  {i+1}. {rid} (dist={results_f['distances'][i]:.4f})\")\n",
    "\n",
    "# Persist to disk and reload\n",
    "faiss_path = str(OUTPUT_DIR / \"faiss_index\")\n",
    "faiss_store.persist(faiss_path)\n",
    "\n",
    "faiss_reloaded = FAISSVectorStore.load(faiss_path)\n",
    "print(f\"\\nReloaded FAISS index: {faiss_reloaded.count()} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9608234",
   "metadata": {},
   "source": [
    "### 5c. High-Level — VectorDB Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad66865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VectorDB class wraps embedding + store in one object.\n",
    "# Here we use the mock embedder; in production use ImageEmbedder().\n",
    "\n",
    "vdb = VectorDB(\n",
    "    embedder=embedder,\n",
    "    store_backend=\"faiss\",\n",
    "    dimension=128,\n",
    ")\n",
    "\n",
    "# Build index from GeoDataFrame\n",
    "vdb.build(\n",
    "    gdf,\n",
    "    text_column=\"scene_narrative\",\n",
    "    metadata_columns=[\"land_use_primary\", \"street_type\"],\n",
    "    id_column=\"image_id\",\n",
    ")\n",
    "\n",
    "print(f\"VectorDB built: {vdb.store.count()} vectors\")\n",
    "\n",
    "# Search by text\n",
    "search_results = vdb.search(query_text=\"busy commercial street with shops\")\n",
    "print(f\"\\nSearch results for 'busy commercial street with shops':\")\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a910fc4",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Full Pipeline (Requires GPU + API Key)\n",
    "\n",
    "The convenience functions in `geoai_vlm.pipeline` chain everything together.\n",
    "These cells require:\n",
    "- A valid **Mapillary API key**\n",
    "- A **GPU** with the Qwen3-VL model weights\n",
    "\n",
    "Skip this section if running on CPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aeb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MLY_API_KEY = os.environ.get(\"MLY_API_KEY\", \"YOUR_MAPILLARY_API_KEY\")\n",
    "HAS_KEY = MLY_API_KEY != \"YOUR_MAPILLARY_API_KEY\"\n",
    "\n",
    "if HAS_KEY:\n",
    "    print(\"Mapillary API key detected — full pipeline cells will run.\")\n",
    "else:\n",
    "    print(\"No API key set. Set MLY_API_KEY to run the cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- embed_place: download, describe, AND embed in one call ---\n",
    "if HAS_KEY:\n",
    "    from geoai_vlm import embed_place\n",
    "\n",
    "    gdf_embedded = embed_place(\n",
    "        place_name=\"Sultanahmet, Istanbul\",\n",
    "        mly_api_key=MLY_API_KEY,\n",
    "        buffer_m=100,\n",
    "        max_images=20,\n",
    "        model_name=\"Qwen/Qwen3-VL-Embedding-2B\",\n",
    "        output_dir=OUTPUT_DIR / \"embed_place\",\n",
    "    )\n",
    "    print(f\"Embedded {len(gdf_embedded)} images\")\n",
    "    print(f\"Embedding dim: {len(gdf_embedded['embedding'].iloc[0])}\")\n",
    "else:\n",
    "    print(\"Skipped — no API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17269f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- cluster_descriptions: cluster a GeoDataFrame or parquet file ---\n",
    "if HAS_KEY:\n",
    "    from geoai_vlm import cluster_descriptions\n",
    "\n",
    "    gdf_clust = cluster_descriptions(\n",
    "        gdf_embedded,\n",
    "        n_clusters=5,\n",
    "        embedding_columns=[\"scene_narrative\", \"semantic_tags\", \"place_character\"],\n",
    "    )\n",
    "    print(gdf_clust[\"cluster\"].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"Skipped — no API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- analyze_spatial: run Moran's I on the clustered data ---\n",
    "if HAS_KEY:\n",
    "    from geoai_vlm import analyze_spatial\n",
    "\n",
    "    spatial = analyze_spatial(gdf_clust, k_neighbors=8)\n",
    "\n",
    "    print(\"Global Moran's I:\")\n",
    "    for cid, mr in spatial[\"global\"].items():\n",
    "        print(f\"  Cluster {cid}: I={mr.I:.4f}, p={mr.p_value:.4f}\")\n",
    "else:\n",
    "    print(\"Skipped — no API key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- build_search_index + search_similar ---\n",
    "if HAS_KEY:\n",
    "    from geoai_vlm import build_search_index, search_similar\n",
    "\n",
    "    vdb = build_search_index(\n",
    "        gdf_embedded,\n",
    "        store_backend=\"faiss\",\n",
    "        text_column=\"scene_narrative\",\n",
    "        metadata_columns=[\"land_use_primary\", \"street_type\"],\n",
    "    )\n",
    "\n",
    "    hits = search_similar(vdb, query_text=\"historic mosque with minarets\", n_results=5)\n",
    "    print(\"Search results:\")\n",
    "    display(hits)\n",
    "else:\n",
    "    print(\"Skipped — no API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0edd2a",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Module | Key Class / Function | What it Does |\n",
    "|---|---|---|\n",
    "| `preparation` | `parse_vlm_descriptions`, `merge_data_sources`, `build_embedding_text` | Parse, merge, and prepare data |\n",
    "| `embedding` | `ImageEmbedder` | Multimodal embedding (Qwen3-VL-Embedding) |\n",
    "| `clustering` | `SemanticClusterer`, `ClusterConfig` | K-Means + TF-IDF keywords |\n",
    "| `spatial` | `SpatialAnalyzer`, `MoranResult` | Moran's I (global + local LISA) |\n",
    "| `vectorstore` | `VectorDB`, `ChromaVectorStore`, `FAISSVectorStore` | Vector search |\n",
    "| `visualization` | `plot_cluster_map`, `plot_lisa_map`, `generate_report` | Charts & reports |\n",
    "| `pipeline` | `embed_place`, `cluster_descriptions`, `analyze_spatial`, `build_search_index`, `search_similar` | One-liner convenience functions |\n",
    "\n",
    "For the core image download and VLM description pipeline, see **`demo_geoai_vlm.ipynb`**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
